{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecccbe9-68c1-4912-ad6c-3c8c43a10d09",
   "metadata": {},
   "source": [
    "# transformers推理模型\n",
    "\n",
    "## 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49e47da-07fb-45b5-b5c7-57188fa3613e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebcb86776334c538935cdc4e9667181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/root/autodl-tmp/Qwen3-8B\"\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",  # 自动选择适合的张量数据类型\n",
    "    device_map=\"auto\"    # 自动将模型分配到可用的设备（如 GPU）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9f101-e5fc-4fbb-8f76-20fbba6b555a",
   "metadata": {},
   "source": [
    "## 输入提示准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c341319-4fe8-41fe-8a8d-47aa7e2fec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 准备模型输入\n",
    "prompt = \"给我简单介绍一下大模型\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,  # 不立即分词，而是生成完整的文本模板\n",
    "    add_generation_prompt=True,  # 添加生成提示（用于引导模型生成内容）\n",
    "    enable_thinking=True  # 启用思考模式，默认为 True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c27d05-2dc7-4429-8c57-84f9337a7e99",
   "metadata": {},
   "source": [
    "## 文本生成处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f6898e-0de9-4ea0-bf27-e7a2b1ea4de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户让我简单介绍一下大模型。首先，我需要确定“大模型”具体指的是什么。通常来说，大模型指的是大规模预训练语言模型，比如像GPT、BERT、T5这些。不过用户可能对技术细节不太熟悉，所以得用简单易懂的语言解释。\n",
      "\n",
      "接下来，我应该考虑用户的需求。他们可能是刚开始接触AI，或者对大模型的应用场景感兴趣。需要区分清楚大模型的基本概念、特点、应用场景以及优缺点。可能还需要提到一些常见的例子，比如GPT-3、BERT等，这样用户更容易理解。\n",
      "\n",
      "另外，用户可能想知道大模型和传统模型的区别，比如参数量、训练数据量、处理任务的能力等。需要强调大模型的规模带来的优势，比如多任务处理、迁移学习能力，但也要提到计算资源和能耗的问题。\n",
      "\n",
      "还要注意避免使用太多专业术语，保持口语化。比如“参数量”可以解释为模型内部的“变量数量”，“预训练”可以理解为在大量数据上先学习通用知识。同时，可能需要提到实际应用，比如聊天机器人、文本生成、翻译等，让用户知道大模型的实际用途。\n",
      "\n",
      "另外，用户可能没有说出来的深层需求是想了解大模型的前景或者如何应用，所以在结尾可以简要提到其影响和未来趋势，比如更高效的模型、更广泛的应用场景等。但要注意不要太过深入，保持简洁。\n",
      "\n",
      "最后，检查有没有遗漏的重要点，比如大模型的训练过程、微调、推理等，但可能不需要太详细。保持回答结构清晰，分点说明，让用户一目了然。\n",
      "</think>\n",
      "\n",
      "大模型（Large Model）通常指那些参数量巨大、训练数据海量、具有强大语言理解和生成能力的AI模型。它们通过在大量文本数据上进行预训练，学习语言的通用规律和知识，从而能够完成多种复杂的任务，比如：\n",
      "\n",
      "### 核心特点：\n",
      "1. **参数量庞大**：通常拥有数十亿甚至数千亿个参数（模型内部的“变量”），这使得模型能捕捉更复杂的语言模式。\n",
      "2. **预训练+微调**：先在海量文本上学习通用知识（预训练），再针对具体任务（如问答、翻译）进行微调。\n",
      "3. **多任务能力**：能处理文本生成、翻译、摘要、问答、代码编写等多种任务，无需针对每个任务单独训练模型。\n",
      "\n",
      "### 应用场景：\n",
      "- **聊天机器人**（如ChatGPT、通义千问）\n",
      "- **文本生成**（如写文章、故事、诗歌）\n",
      "- **智能客服**（自动回答用户问题）\n",
      "- **代码辅助**（生成或修复代码）\n",
      "- **数据分析**（从文本中提取信息）\n",
      "\n",
      "### 优势：\n",
      "- **泛化能力强**：能理解并生成与训练数据相似的高质量文本。\n",
      "- **迁移学习**：通过少量数据即可快速适应新任务。\n",
      "\n",
      "### 挑战：\n",
      "- **计算资源需求高**：训练和推理需要强大的算力和存储。\n",
      "- **能耗大**：运行成本高，可能对环境造成负担。\n",
      "- **潜在风险**：可能生成错误信息或违反伦理的内容。\n",
      "\n",
      "### 常见例子：\n",
      "- **GPT系列**（如GPT-3、GPT-4）\n",
      "- **BERT**（用于自然语言理解）\n",
      "- **T5**（多任务文本到文本生成）\n",
      "- **通义千问**（Qwen）\n",
      "\n",
      "简单来说，大模型就像一个“超级大脑”，通过学习海量知识，能像人类一样理解和生成语言，但需要强大的算力支持。\n"
     ]
    }
   ],
   "source": [
    "# 执行文本生成\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768  # 设置生成的最大新标记数\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# 输出结果将首先包含思考内容（用 <think></think> 标签包裹），然后是实际的回答\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3fcd5-667e-45c3-987d-9a5aaa48cfac",
   "metadata": {},
   "source": [
    "## 禁用思考模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40949a21-2449-4ca0-bda8-29a6f331021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以！下面是一个关于“大模型”的简单介绍：\n",
      "\n",
      "---\n",
      "\n",
      "### 什么是大模型？\n",
      "\n",
      "**大模型**（Large Model），也被称为**大语言模型**（Large Language Model, LLM），是一种基于深度学习的**人工智能模型**，它通过在海量文本数据上进行训练，掌握语言的结构、语义和使用习惯，从而能够完成多种自然语言处理任务。\n",
      "\n",
      "---\n",
      "\n",
      "### 特点：\n",
      "\n",
      "1. **参数量大**：通常拥有数十亿甚至上千亿个参数，这是其“大”的核心特征。\n",
      "2. **语言理解能力强**：可以理解并生成自然语言，包括中文、英文等多种语言。\n",
      "3. **多任务处理能力**：可以完成文本生成、翻译、问答、摘要、代码编写等多种任务。\n",
      "4. **自学习能力**：通过大量数据训练，模型可以不断优化自己的语言理解和生成能力。\n",
      "\n",
      "---\n",
      "\n",
      "### 应用场景：\n",
      "\n",
      "- **智能客服**：自动回答用户问题。\n",
      "- **内容创作**：写文章、写故事、写邮件等。\n",
      "- **编程辅助**：帮助编写代码、调试程序。\n",
      "- **数据分析**：生成报告、解释数据。\n",
      "- **教育辅助**：辅导学习、解答问题。\n",
      "\n",
      "---\n",
      "\n",
      "### 常见的大模型：\n",
      "\n",
      "- **GPT**（由OpenAI开发）\n",
      "- **BERT**（由Google开发）\n",
      "- **ERNIE**（由百度开发）\n",
      "- **通义千问**（Qwen，由阿里云开发）\n",
      "- **LLaMA**（由Meta开发）\n",
      "\n",
      "---\n",
      "\n",
      "### 小结：\n",
      "\n",
      "大模型是人工智能领域的重要突破，它让机器具备了更强的语言理解和生成能力，正在深刻改变我们的生活和工作方式。\n",
      "\n",
      "如果你对某个具体的大模型感兴趣，也可以告诉我，我可以进一步为你介绍！\n"
     ]
    }
   ],
   "source": [
    "prompt = \"给我简单介绍一下大模型\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,  # 不立即分词，而是生成完整的文本模板\n",
    "    add_generation_prompt=True,  # 添加生成提示（用于引导模型生成内容）\n",
    "    enable_thinking=False  # 启用思考模式，默认为 True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 执行文本生成\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768  # 设置生成的最大新标记数\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# 输出结果将首先包含思考内容（用 <think></think> 标签包裹），然后是实际的回答\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8692c3b-ea6e-471e-818e-00f705911147",
   "metadata": {},
   "source": [
    "# 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d08ec-414c-4605-8d83-dea7a4d5e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"/root/autodl-tmp/Qwen3-8B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"请简单介绍一下大模型\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# the result will begin with thinking content in <think></think> tags, followed by the actual response\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
