{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecccbe9-68c1-4912-ad6c-3c8c43a10d09",
   "metadata": {},
   "source": [
    "# Qwen3 Agent实战之解锁模型的智能交互能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235bdbf-3dda-41ce-a455-f5595e75526c",
   "metadata": {},
   "source": [
    "# 一、准备OpenAI API 服务\n",
    "在这里，我使用ollama部署服务，提供一个openai API风格的服务，便于后面Qwen-Agent实践测试\n",
    "## （一）安装 Ollama\n",
    "访问 Ollama 官网，根据你的操作系统（Windows、macOS、Linux）下载安装包。\n",
    "安装完成后，打开终端，输入 ollama --version 验证是否成功。\n",
    "\n",
    "## （二）下载模型\n",
    "运行以下命令拉取模型：\n",
    "```bash\n",
    "ollama pull qwen3:8b\n",
    "```\n",
    "\n",
    "模型会下载到本地（默认路径 ~/.ollama/models），大小约 16GB，需要等待十几分钟。\n",
    "\n",
    "## （三）启动 Ollama 服务\n",
    "输入以下命令启动 API 服务：\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "服务默认监听 http://localhost:11434，可以用浏览器访问这个地址测试（会返回状态信息）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81367e08-8a29-4d4c-83d8-b70d896aaca5",
   "metadata": {},
   "source": [
    "## 4.openai API服务测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec93186-d103-4317-a07b-7d1ddab25b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户问的是“什么是大模型？”，我需要先明确大模型的定义。大模型通常指的是大规模参数模型，比如像GPT-3、BERT这样的模型，它们有数百万到数十亿的参数。不过，用户可能不太清楚参数的具体含义，可能需要用更通俗的语言解释。\n",
      "\n",
      "接下来，我得考虑用户可能的背景。他们可能对AI有一定的了解，但不确定大模型的具体概念，或者完全是个新手。所以，我需要用简单易懂的例子来说明，比如比较小模型和大模型的区别，比如用手机电池容量来比喻参数量。\n",
      "\n",
      "然后，用户可能想知道大模型的应用场景。比如自然语言处理、图像识别、语音助手等，这些都是常见的应用领域。需要举一些实际的例子，比如翻译、写作文、生成图片，这样用户更容易理解。\n",
      "\n",
      "另外，用户可能关心大模型的优势和挑战。优势方面，大模型在多个任务上表现优异，能够处理复杂任务，但缺点是需要大量算力和数据，训练成本高，还有可能产生偏见或错误信息。这部分需要客观说明，避免过于技术化。\n",
      "\n",
      "还要注意用户可能的深层需求。他们也许想知道为什么大模型重要，或者如何使用这些模型。所以，在回答中可以提到实际应用案例，比如像通义千问这样的模型，帮助用户理解大模型的实际价值。\n",
      "\n",
      "最后，确保回答结构清晰，分点说明，让用户容易跟随。同时，语言要口语化，避免专业术语过多，保持自然流畅。检查是否有遗漏的重要点，比如大模型的发展趋势或伦理问题，但可能用户暂时不需要这些，所以保持回答简洁但全面。\n",
      "</think>\n",
      "\n",
      "大模型（Large Model）是指拥有大量参数（通常上亿甚至数十亿）的深度学习模型。这类模型通过海量数据训练，能够捕捉复杂的模式和关系，从而在多种任务中表现出强大的能力。以下是关于大模型的几个关键点：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 什么是“参数”？**\n",
      "参数是模型内部用于学习数据特征的变量。参数越多，模型的表达能力越强，可以处理更复杂的任务。例如：\n",
      "- **小模型**：参数量可能只有几百万，适合简单任务（如分类）。\n",
      "- **大模型**：参数量可达数十亿，能处理更复杂的任务（如理解自然语言、生成文本等）。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 大模型的核心特点**\n",
      "- **强大的泛化能力**：能将\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 初始化客户端，指向 Ollama 的本地服务\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Ollama API 地址\n",
    "    api_key=\"qwen3-8b\"  # Ollama 默认无需真实 API Key，填任意值即可\n",
    ")\n",
    "\n",
    "# 发送请求\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen3:8B\",  # 指定模型\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个智能AI助手。\"},\n",
    "        {\"role\": \"user\", \"content\": \"你好，什么是大模型？\"}\n",
    "    ],\n",
    "    temperature=0.7,  # 控制生成多样性\n",
    "    max_tokens=512    # 最大生成 token 数\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338d2ec-0e2b-4995-aed8-5555237fe5e9",
   "metadata": {},
   "source": [
    "# 二、安装Qwen-Agent\n",
    "\n",
    "```bash\n",
    "pip install -U \"qwen-agent[gui,rag,code_interpreter,mcp]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00ec9a-e4c6-4705-a0ee-2c1d7bd04451",
   "metadata": {},
   "source": [
    "# 三、Qwen-Agent 使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dcb0994-7b05-470b-b47b-e7bbe674e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 21:36:15,215 - mcp_manager.py - 141 - INFO - Initializing MCP tools from mcp servers: ['time', 'fetch']\n",
      "2025-06-02 21:36:15,219 - mcp_manager.py - 370 - INFO - Initializing a MCP stdio_client, if this takes forever, please check the config of this mcp server: time\n",
      "2025-06-02 21:36:15,844 - mcp_manager.py - 370 - INFO - Initializing a MCP stdio_client, if this takes forever, please check the config of this mcp server: fetch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'assistant', 'content': '<think>\\nOkay, the user wants information about the latest developments in Qwen from the provided URL. First, I need to fetch the content from the website. The URL is https://qwenlm.github.io/blog/. Since the user mentioned \"介绍 Qwen 的最新发展\", which translates to introducing the latest developments of Qwen, I should retrieve the blog content.\\n\\nI\\'ll use the fetch function to get the webpage. The parameters required are the URL, and maybe set max_length to 5000 as default. Since the user might want a summary, I\\'ll fetch the content and then extract the relevant information. However, the fetch function can return either raw HTML or markdown. Since the user might prefer a clean summary, I\\'ll set raw to false to get markdown content, which is easier to parse.\\n\\nWait, but the fetch function\\'s raw parameter defaults to false, so maybe I don\\'t need to specify it. But to be safe, maybe include it. Also, check if the URL is correct. The user provided the URL, so I\\'ll proceed. Once fetched, I can then process the content to find sections related to Qwen\\'s latest updates. If the content is in Chinese, I\\'ll need to ensure the code interpreter or other tools can handle it, but since the user is asking in Chinese, the fetched content is likely in Chinese. \\n\\nAlternatively, maybe the blog has specific sections or posts about Qwen\\'s developments. After fetching, I\\'ll look for keywords like \"Qwen\", \"最新发展\", \"更新\" etc. to extract the relevant parts. If the content is too long, the max_length of 5000 should cover most of the page. If not, maybe adjust, but the default is 5000. \\n\\nSo the plan is: fetch the URL, then process the returned content to summarize the latest Qwen developments. Since the user might need a concise answer, I\\'ll make sure to highlight key points from the fetched data. If there\\'s any specific information needed, like release dates, features, or improvements, I\\'ll extract those. \\n\\nI should also check if there are any other functions needed. The time functions aren\\'t necessary here unless the blog mentions time-related info, but the user\\'s query is about developments, so likely not. The code interpreter might be useful if there\\'s code in the blog, but the user is asking for an introduction, so maybe not. \\n\\nTherefore, the main step is to fetch the URL and then summarize the relevant sections. Let\\'s proceed with the fetch function first.\\n</think>\\n\\n'}, {'role': 'assistant', 'content': '', 'function_call': {'name': 'fetch', 'arguments': '{\"url\": \"https://qwenlm.github.io/blog/\", \"max_length\": 5000, \"raw\": false}'}}, {'role': 'function', 'content': 'Tool fetch does not exists.', 'name': 'fetch'}, {'role': 'assistant', 'content': \"<think>\\nOkay, the user asked me to introduce the latest developments of Qwen, and I tried to use the fetch function to get the blog content. But the system said the fetch tool doesn't exist. Hmm, maybe I made a mistake in the tool name. Let me check the available tools again.\\n\\nLooking back, the tools provided include time-related functions and code_interpreter. Wait, there's no fetch function listed. Oh, right, the user mentioned that the fetch tool is supposed to grant internet access, but maybe it's not available here. Since I can't use fetch, I need to find another way. \\n\\nSince I don't have internet access, I can't retrieve the latest blog content. I should inform the user that I can't access the internet and suggest they check the official website or documentation for the latest updates. Also, maybe I can mention known developments of Qwen based on my training data up to 2024, but I should clarify that it's not the latest info from the blog. Let me structure the response to be helpful and honest about the limitations.\\n</think>\\n\\n我目前无法直接访问互联网，因此无法实时获取Qwen的最新发展动态。建议您访问通义千问官方网站或相关技术博客，以获取最准确和最新的信息。\\n\\n不过，根据我所掌握的知识（更新至2024年），Qwen系列模型持续在以下几个方向取得进展：\\n1. 多模态能力增强 - 支持文本、图像、音频等多类型数据处理\\n2. 代码生成优化 - 提升编程相关任务的准确性和实用性\\n3. 大型语言模型架构创新 - 不断优化参数规模和计算效率\\n4. 应用场景扩展 - 在更多行业领域实现定制化解决方案\\n\\n如需了解具体技术细节或最新动态，请查看官方技术博客或联系相关技术人员。\"}]\n"
     ]
    }
   ],
   "source": [
    "from qwen_agent.agents import Assistant\n",
    "\n",
    "# 定义 LLM\n",
    "llm_cfg = {\n",
    "    'model': 'qwen3:8b',\n",
    "    # 使用自定义 OpenAI API 兼容的端点\n",
    "    'model_server': 'http://localhost:11434/v1',  # api_base\n",
    "    'api_key': 'EMPTY',\n",
    "}\n",
    "\n",
    "# 定义工具\n",
    "tools = [\n",
    "    {'mcpServers': {\n",
    "        'time': {\n",
    "            'command': 'uvx',\n",
    "            'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n",
    "        },\n",
    "        \"fetch\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\"mcp-server-fetch\"]\n",
    "        }\n",
    "    }},\n",
    "    'code_interpreter',  # 内置工具\n",
    "]\n",
    "\n",
    "# 定义代理\n",
    "bot = Assistant(llm=llm_cfg, function_list=tools)\n",
    "\n",
    "# 流式生成\n",
    "messages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ 介绍 Qwen 的最新发展'}]\n",
    "for responses in bot.run(messages=messages):\n",
    "    pass\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717dd8a-eeb4-4e4b-bc11-df7624b80097",
   "metadata": {},
   "source": [
    "## 第二轮测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a1ef5ff-1c89-44cb-af4f-49658bbc0ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'assistant', 'content': '<think>\\n好的，用户问的是“明天的日期是？”。我需要先确定用户所在的时区，因为不同的时区日期可能不同。根据提供的工具，可以使用time-get_current_time函数，但需要指定时区。然而，用户没有提供时区信息，所以按照函数描述，默认会使用Asia/Shanghai作为本地时区。\\n\\n首先，我需要获取当前的时间，然后计算明天的日期。不过，可能更直接的是直接获取明天的日期。但根据函数功能，time-get_current_time可能只返回当前时间，所以可能需要先获取当前时间，然后加一天。或者，可能需要另一个方法，但工具中没有日期加减的函数。因此，可能需要调用time-get_current_time函数，然后在结果上加一天。\\n\\n不过，用户的问题是关于日期的，而time-get_current_time返回的是当前时间，包括日期和时间。所以，如果当前时间是2023-10-05，那么明天的日期就是2023-10-06。因此，我需要先调用time-get_current_time，使用Asia/Shanghai时区，然后将日期加一天，再返回结果。\\n\\n但用户可能只需要日期部分，不需要时间。因此，步骤应该是：获取当前时间，然后计算明天的日期。不过，可能需要考虑时区的问题，确保正确性。例如，如果用户所在的时区是UTC+8，那么当前时间是下午某个时间，而明天的日期可能已经过了。不过，通常来说，获取当前时间后，加一天的日期部分即可。\\n\\n所以，我需要调用time-get_current_time函数，参数时区为Asia/Shanghai，然后处理返回的结果，得到明天的日期。或者，可能直接调用该函数，并返回当前日期的下一天。但具体实现可能需要代码来处理日期计算。不过，工具中的函数可能直接返回当前时间，因此需要手动计算。\\n\\n或者，可能用户的问题只需要当前日期的下一天，而不需要具体时间。因此，调用time-get_current_time函数，得到当前日期，然后加一天。例如，如果当前日期是2023-10-05，那么明天就是2023-10-06。这时候可能需要使用code_interpreter来执行日期计算。例如，用Python的datetime模块来处理。\\n\\n不过，根据工具中的函数，可能需要先调用time-get_current_time来获取当前时间，然后进行计算。但可能更高效的是直接使用代码解释器来计算明天的日期。比如，如果当前时间是2023-10-05，那么明天就是2023-10-06。但需要确定当前时间的正确性。\\n\\n因此，可能需要先调用time-get_current_time函数，获取当前日期，然后在代码中计算明天的日期。或者，如果系统时间已经正确，可能直接获取当前日期并加一天。比如，使用Python的datetime模块，获取当前日期，然后加一天。\\n\\n最终，我需要确定如何正确获取明天的日期，并且确保时区正确。因此，可能需要分两步：首先获取当前时间，然后计算明天的日期。或者，直接使用代码解释器来计算，假设当前时间是正确的。例如，使用code_interpreter执行代码来获取明天的日期。\\n</think>\\n\\n'}, {'role': 'assistant', 'content': '', 'function_call': {'name': 'time-get_current_time', 'arguments': '{\"timezone\": \"Asia/Shanghai\"}'}}, {'role': 'function', 'content': '{\\n  \"timezone\": \"Asia/Shanghai\",\\n  \"datetime\": \"2025-06-02T21:38:20+08:00\",\\n  \"is_dst\": false\\n}', 'name': 'time-get_current_time'}, {'role': 'assistant', 'content': '<think>\\n好的，用户问的是“明天的日期是？”，我需要先确定当前日期，然后计算明天的日期。用户可能想知道具体的日期，比如月份和日，可能还需要考虑时区的问题。\\n\\n首先，我调用了time-get_current_time函数，获取当前上海的时间，因为用户没有指定时区，所以默认使用上海时间。返回的结果是2025年6月2日21:38:20+08:00，且不是夏令时。这说明今天是6月2日，那么明天应该是6月3日。但需要确认一下，当前时间是21:38，所以明天的日期应该是6月3日，对吗？\\n\\n不过用户可能只需要日期部分，不需要具体时间。因此，直接回答明天的日期是2025年6月3日即可。应该不需要考虑夏令时的影响，因为返回的is_dst是false，说明当前时间是标准时间。所以最终的结论是明天是6月3日。\\n</think>\\n\\n明天的日期是2025年6月3日。'}]\n"
     ]
    }
   ],
   "source": [
    "# 流式生成\n",
    "messages = [{'role': 'user', 'content': '明天的日期是？'}]\n",
    "for responses in bot.run(messages=messages):\n",
    "    pass\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a6c31-02e8-4d39-9097-95ecd3aa6de2",
   "metadata": {},
   "source": [
    "## 第三轮测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fefa86e0-1ec6-466f-8949-46f96a6286fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'assistant', 'content': '<think>\\n好的，用户让我计算2的10次方。这个问题看起来不难，但我要仔细确认用户的需求。首先，2的10次方就是2乘以自己10次，结果应该是1024。不过，用户可能希望看到详细的步骤，或者他们可能对指数运算不太确定。也有可能用户是在测试我的计算能力，或者他们需要这个结果用于某个具体的应用场景，比如编程、数学问题或者日常计算。\\n\\n我需要考虑是否需要用代码来执行这个计算，以确保准确性。虽然手动计算很容易，但使用代码解释器可以验证结果，并且展示过程。比如，用Python的话，可以直接写2**10，或者用循环相乘。不过，对于这样的简单问题，直接给出结果可能更高效。但用户可能希望看到更详细的解释，或者他们可能想了解不同的计算方法。\\n\\n另外，用户可能没有明确说明是否需要步骤，所以最好提供结果的同时，简要说明过程。比如，2^10等于1024，这是常见的计算机科学中的数值，比如1024字节等于1千字节。这样用户不仅得到答案，还能了解其应用场景。不过，如果用户只需要结果，直接回答即可。但为了全面，我应该提供结果和简要的解释，确保满足不同需求。\\n</think>\\n\\n2的10次方等于1024。\\n\\n计算过程：\\n2 × 2 × 2 × 2 × 2 × 2 × 2 × 2 × 2 × 2\\n= 1024\\n\\n这个结果在计算机科学中很有意义，例如：\\n- 1024字节 = 1千字节（KB）\\n- 1024兆字节（MB）= 1千兆字节（GB）'}]\n"
     ]
    }
   ],
   "source": [
    "# 流式生成\n",
    "messages = [{'role': 'user', 'content': '计算2的10次方'}]\n",
    "for responses in bot.run(messages=messages):\n",
    "    pass\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77913bd1-0832-4779-874a-4b5ac5d5272b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
