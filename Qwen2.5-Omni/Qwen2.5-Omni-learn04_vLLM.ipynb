{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e849618-7120-4d75-b59a-2dbf7d24e8e4",
   "metadata": {},
   "source": [
    "# 使用vLLM部署\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9698e-ff32-408c-b24d-0380d7cc8e70",
   "metadata": {},
   "source": [
    "# 一、安装相关依赖\n",
    "\n",
    "在开始之前，请确保已将所需的依赖项安装到系统上：\n",
    "```bash\n",
    "pip install git+https://github.com/huggingface/transformers@d40f54fc2f1524458669048cb40a8d0286f5d1d2\n",
    "pip install accelerate\n",
    "pip install qwen-omni-utils\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e876bbf-da9b-4c29-924f-3dcce5dae591",
   "metadata": {},
   "source": [
    "安装vllm\n",
    "使用vLLM进行Qwen2.5-Omni的快速部署和推理时，需要从官方提供的源码构建vLLM以获取对Qwen2.5-Omni支持\n",
    "```bash\n",
    "git clone -b qwen2_omni_public_v1 https://github.com/fyabc/vllm.git\n",
    "cd vllm\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c40c91-4d15-49c9-a2d7-4148c4509650",
   "metadata": {},
   "source": [
    "# 二、下载模型文件\n",
    "我们将模型下载到本地，方便离线推理;用modelscope库的snapshot_download函数下载模型。 \n",
    "Qwen/Qwen2.5-Omni-7B是模型在平台上的名字， cache_dir是你要把模型存到哪里，你可以自己改，revision='master’表示下最新版本。\n",
    "\n",
    "下载的时候，网络要稳定，存储路径别写错，不然就下不了。代码写好后，直接执行，模型就开始下载了，下载时间要看网速。 \n",
    "注意：也可以直接将上面代码封装到python文件中（例如：download.py）直接执行python download.py即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea1eed-6dc7-4d38-9ca8-ce4a2193ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('Qwen/Qwen2.5-Omni-7B', cache_dir='/root/autodl-tmp', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426ac01-e25b-47b1-9cb7-ced35629919f",
   "metadata": {},
   "source": [
    "# 二、本地离线推理\n",
    "使用vLLM来本地离线推理Qwen2.5-Omni，目前官方只支持vllm的thinker部分，因此输出的模型只能是文本。官方将在不久的未来支持模型的其他部分以实现音频输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80609b85-83cd-44f1-ac09-0349dd90d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import Qwen2_5OmniProcessor  # 导入用于处理多模态数据的处理器\n",
    "from vllm import LLM, SamplingParams  # 导入 vLLM 模型和采样参数类\n",
    "from qwen_omni_utils import process_mm_info  # 导入用于处理多模态信息的工具函数\n",
    "\n",
    "# 禁用 vLLM engine v1（目前不支持）\n",
    "os.environ['VLLM_USE_V1'] = '0'\n",
    "\n",
    "# 模型路径，指向预训练模型的存储位置\n",
    "MODEL_PATH = \"/root/autodl-tmp/Qwen/Qwen2.5-Omni-7B\"\n",
    "\n",
    "# 初始化 LLM 模型\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,  # 指定模型路径\n",
    "    trust_remote_code=True,  # 允许加载远程代码（例如模型配置）\n",
    "    gpu_memory_utilization=0.9,  # 设置 GPU 内存利用率（0.9 表示使用 90% 的 GPU 内存）\n",
    "    tensor_parallel_size=torch.cuda.device_count(),  # 设置张量并行的设备数量（根据 GPU 数量自动调整）\n",
    "    limit_mm_per_prompt={'image': 1, 'video': 1, 'audio': 1},  # 限制每条提示中每种模态的最大数量（图像、视频、音频各 1 个）\n",
    "    seed=1234,  # 设置随机种子以确保结果可复现\n",
    ")\n",
    "\n",
    "# 定义采样参数\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=1e-6,  # 设置温度参数（接近 0 表示更确定性的输出）\n",
    "    max_tokens=512,  # 设置生成的最大 token 数量\n",
    ")\n",
    "\n",
    "# 初始化多模态处理器\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 定义对话消息，包括系统角色和用户角色\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\",\n",
    "    },  # 系统角色的描述，定义模型的行为和能力\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": \"/root/autodl-tmp/draw.mp4\"},\n",
    "        ],\n",
    "    },  # 用户角色的消息，包含一个视频文件路径\n",
    "]\n",
    "\n",
    "# 使用处理器将对话消息转换为模型输入的文本模板\n",
    "# 并添加生成提示\n",
    "text = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,  # 不进行 token 化（直接使用文本）\n",
    "    add_generation_prompt=True,  # 添加生成提示\n",
    ")\n",
    "\n",
    "# 使用工具函数处理多模态信息\n",
    "# 提取音频、图像和视频数据\n",
    "# 参数 use_audio_in_video=True 表示从视频中提取音频\n",
    "audios, images, videos = process_mm_info(messages, use_audio_in_video=True)\n",
    "\n",
    "# 构建模型输入字典\n",
    "inputs = {\n",
    "    'prompt': text[0],  # 文本提示\n",
    "    'multi_modal_data': {},  # 多模态数据\n",
    "    \"mm_processor_kwargs\": {\n",
    "        \"use_audio_in_video\": True,  # 传递多模态处理器的参数\n",
    "    },\n",
    "}\n",
    "\n",
    "# 根据提取的多模态数据，填充输入字典\n",
    "if images is not None:\n",
    "    inputs['multi_modal_data']['image'] = images  # 添加图像数据\n",
    "if videos is not None:\n",
    "    inputs['multi_modal_data']['video'] = videos  # 添加视频数据\n",
    "if audios is not None:\n",
    "    inputs['multi_modal_data']['audio'] = audios  # 添加音频数据\n",
    "\n",
    "# 使用 LLM 模型生成输出\n",
    "outputs = llm.generate(inputs, sampling_params=sampling_params)\n",
    "\n",
    "# 打印生成的文本结果\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9e483-dda7-4d71-8e23-3a1115eb2354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c177d-05cd-4414-93d9-0240b71eb126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab332a-28e6-4bbb-8cea-0657e5293c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
