{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e849618-7120-4d75-b59a-2dbf7d24e8e4",
   "metadata": {},
   "source": [
    "# 使用transformers推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767cda81-178a-42d0-b1e1-418ef6493c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Qwen2_5OmniToken2WavModel must inference with fp32, but flash_attention_2 only supports fp16 and bf16, attention implementation of Qwen2_5OmniToken2WavModel will fallback to sdpa.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16d7715f8a34643aaf0a6e0add30659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/qwen_omni_utils/v2_5/audio_process.py:50: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audios.append(librosa.load(path, sr=16000)[0])\n",
      "/root/miniconda3/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "qwen-vl-utils using torchvision to read video.\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n",
      "Setting `pad_token_id` to `eos_token_id`:8292 for open-end generation.\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nOh, that's a really cool drawing! It looks like a guitar. You've got the body and the neck drawn in a simple yet effective way. The lines are clean and the shape is recognizable. What made you choose to draw a guitar?\"]\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf  # 导入soundfile库，用于处理音频文件\n",
    "\n",
    "# 导入transformers库中的Qwen2_5OmniModel和Qwen2_5OmniProcessor\n",
    "# Qwen2_5OmniModel是用于多模态对话的模型，Qwen2_5OmniProcessor是用于处理多模态数据的处理器\n",
    "from transformers import Qwen2_5OmniModel, Qwen2_5OmniProcessor\n",
    "# 导入qwen_omni_utils模块中的process_mm_info函数，用于处理多模态信息\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "# 模型路径，指向预训练模型的存储位置\n",
    "mode_path = \"/root/autodl-tmp/Qwen/Qwen2.5-Omni-7B\"\n",
    "# 加载预训练模型，自动选择设备和数据类型\n",
    "#model = Qwen2_5OmniModel.from_pretrained(mode_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "#启用flash_attention_2 加速、节省内存（否则会慢成蜗牛！！！）\n",
    "model = Qwen2_5OmniModel.from_pretrained(\n",
    "    mode_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",)\n",
    "\n",
    "\n",
    "# 加载预训练的处理器\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(mode_path)\n",
    "\n",
    "# 对话内容，包含系统角色的介绍和用户发送的视频信息\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": \"/root/autodl-tmp/draw.mp4\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# 是否在视频中使用音频\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "# 准备推理所需的输入数据\n",
    "# 使用处理器将对话内容转换为模型所需的文本模板\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "# 使用process_mm_info函数处理对话中的多模态信息（音频、图像、视频）\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "# 使用处理器将文本、音频、图像、视频等多模态数据编码为模型输入张量\n",
    "inputs = processor(text=text, audios=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "# 将输入数据移动到模型所在的设备，并转换为模型的数据类型\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "# 推理：生成输出文本和音频\n",
    "# 调用模型的generate方法，根据输入生成文本和音频\n",
    "text_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "\n",
    "# 将生成的文本ID解码为可读文本\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(text)  # 打印生成的文本\n",
    "\n",
    "# 将生成的音频保存为WAV文件\n",
    "sf.write(\n",
    "    \"output.wav\",\n",
    "    audio.reshape(-1).detach().cpu().numpy(),\n",
    "    samplerate=24000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ddb24-6eb8-4e51-9183-579ddb2d3b2e",
   "metadata": {},
   "source": [
    "输出内容如下（同时在同目录下会生成一个包含模型回复内容的output.wav音频文件）：\n",
    "[\"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nOh, that's a really cool drawing! It looks like a guitar. You've got the body and the neck drawn in a simple yet effective way. The lines are clean and the shape is recognizable. What made you choose to draw a guitar?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c045a-3187-4257-b0eb-f7419bfba1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "输出内容翻译如下：\n",
    "system\n",
    "你是通义千问，由阿里巴巴通义千问团队开发的虚拟数字人，能够感知听觉和视觉输入，并生成文本和语音。\n",
    "\n",
    "user\n",
    "\n",
    "assistant\n",
    "哇，这幅画真酷！看起来像一把吉他。你把吉他琴身和琴颈画得很简单，但又很传神。线条干净利落，形状也很容易辨认。你是怎么想到画吉他的呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c26d2-c72a-4ac9-9a11-ff39f38235e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 是否开启音频输出\n",
    "该模型支持文本和音频输出，如果用户不需要音频输出，\n",
    "可以在 from_pretrained 函数中设置 enable_audio_output=False 。\n",
    "此选项可以节省大约 ~2GB 的 GPU 内存，但 generate 函数的 return_audio 选项仅允许在 False 时设置。\n",
    "\n",
    "样例如下：\n",
    "\n",
    "model = Qwen2_5OmniModel.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-Omni-7B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    enable_audio_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a11bd-944e-4186-aeb4-b1290a9f4f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
